{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import urllib \n",
    "import shutil\n",
    "import azureml\n",
    "\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace, Run\n",
    "from azureml.core import Environment, ScriptRunConfig\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "dataset = ws.datasets[\"Refactored Classification Dataset\"]\n",
    "mount_ctx = dataset.as_mount()  \n",
    "# dataset = Dataset.get_by_name(ws, name='Refactored Classification Dataset')\n",
    "# dataset.download(target_path='.', overwrite=False)\n",
    "cluster_name = \"gpu-cluster\"\n",
    "compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "print('Found existing compute target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newenv = Environment.get(workspace=ws, name=\"Custom-threshold-testing-tf24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = args = ['--data-folder', mount_ctx,\n",
    "                    '--batch-size', 64]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=\"./thresholding\",\n",
    "                      script='softmax_calibration.py',\n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=newenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Experiment(workspace=ws, name='softmax_calibration').submit(src)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
